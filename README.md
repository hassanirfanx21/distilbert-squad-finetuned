# 🤖 DistilBERT Fine-tuned for Question Answering

A fine-tuned DistilBERT model for extractive question answering, trained on the Stanford Question Answering Dataset (SQuAD) v2.0.


## 📋 About

This project fine-tunes a DistilBERT model for question answering tasks using the SQuAD v2.0 dataset from Hugging Face. DistilBERT is a lighter, faster version of BERT that maintains 97% of BERT's performance while being 60% smaller and faster.

## 🎯 Features

- **Model**: DistilBERT (uncased) - Hugging Face pre-trained model
- **Dataset**: SQuAD v2.0 from Hugging Face datasets
- **Task**: Extractive Question Answering
- **Framework**: Hugging Face Transformers

## 🚀 Usage

Run the `squad.ipynb` notebook to train the model. The notebook includes all necessary code for:
- Loading the pre-trained DistilBERT model
- Preprocessing the SQuAD v2.0 dataset
- Fine-tuning the model
- Saving the trained model

## 📊 Dataset

**SQuAD v2.0**: ~130k training examples and ~12k validation examples with questions, contexts, and answers (including unanswerable questions).

## 📄 License

MIT License - see [LICENSE](LICENSE) file for details.

## 👨‍💻 Author

Hassan Irfan - [@hassanirfanx21](https://github.com/hassanirfanx21)

---

⭐ **Star this repository if you found it helpful!**
