{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ DistilBERT Fine-tuning for Question Answering on SQuAD Dataset\n",
        "\n",
        "This notebook demonstrates how to fine-tune a DistilBERT model for question answering using the Stanford Question Answering Dataset (SQuAD) v2.0. DistilBERT is a smaller, faster version of BERT that retains 97% of BERT's performance while being 60% smaller and 60% faster.\n",
        "\n",
        "## üìã Overview\n",
        "- **Model**: DistilBERT (uncased) - A distilled version of BERT\n",
        "- **Dataset**: SQuAD v2.0 - Stanford Question Answering Dataset\n",
        "- **Task**: Extractive Question Answering\n",
        "- **Framework**: Hugging Face Transformers\n",
        "\n",
        "## üéØ What This Notebook Does\n",
        "1. Loads a pre-trained DistilBERT model and tokenizer\n",
        "2. Preprocesses the SQuAD v2.0 dataset for question answering\n",
        "3. Fine-tunes the model on the training data\n",
        "4. Saves the fine-tuned model for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcO7bstCNZaO"
      },
      "outputs": [],
      "source": [
        "# üì¶ Install required libraries\n",
        "!pip install -q datasets transformers accelerate\n",
        "\n",
        "print(\"‚úÖ Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Model and Tokenizer Setup\n",
        "\n",
        "We'll use DistilBERT, which is:\n",
        "- **Faster**: 60% faster inference than BERT\n",
        "- **Smaller**: 40% fewer parameters\n",
        "- **Efficient**: Maintains 97% of BERT's performance\n",
        "- **Versatile**: Pre-trained on a large corpus of English data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# üì• Load Pretrained Tokenizer & Model\n",
        "# ---------------------------------------------\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
        "\n",
        "print(\"üîÑ Loading DistilBERT tokenizer and model...\")\n",
        "\n",
        "# Load tokenizer: converts raw text into BERT-compatible tokens/IDs\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Load DistilBERT model for QA: outputs start and end positions in context\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer loaded successfully!\")\n",
        "print(f\"üìä Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Dataset Loading\n",
        "\n",
        "SQuAD (Stanford Question Answering Dataset) v2.0 contains:\n",
        "- **Training set**: ~130,000 examples\n",
        "- **Validation set**: ~12,000 examples\n",
        "- **Features**: Questions, contexts, and answers (some questions have no answers)\n",
        "- **Task**: Given a question and context, find the answer span in the context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# üìö Load Dataset\n",
        "# ---------------------------------------------\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üîÑ Loading SQuAD v2.0 dataset...\")\n",
        "\n",
        "# Load the SQuAD v2 dataset which includes questions with and without answers\n",
        "dataset = load_dataset(\"squad_v2\")\n",
        "\n",
        "print(\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"üìä Training examples: {len(dataset['train']):,}\")\n",
        "print(f\"üìä Validation examples: {len(dataset['validation']):,}\")\n",
        "\n",
        "# Display a sample\n",
        "print(\"\\nüìñ Sample from dataset:\")\n",
        "sample = dataset[\"train\"][0]\n",
        "print(f\"Question: {sample['question']}\")\n",
        "print(f\"Context: {sample['context'][:200]}...\")\n",
        "print(f\"Answer: {sample['answers']['text'][0] if sample['answers']['text'] else 'No answer'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö° Data Preprocessing\n",
        "\n",
        "The preprocessing function handles several important tasks:\n",
        "\n",
        "1. **Tokenization**: Converts text to tokens that DistilBERT can understand\n",
        "2. **Truncation**: Handles long contexts by splitting them into chunks\n",
        "3. **Answer Mapping**: Maps character-level answer positions to token positions\n",
        "4. **Padding**: Ensures all inputs have the same length for efficient batch processing\n",
        "\n",
        "This is crucial for training the model effectively on the SQuAD dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# ‚ö° Preprocessing Function\n",
        "# ---------------------------------------------\n",
        "def preprocess(example):\n",
        "    \"\"\"\n",
        "    Preprocesses examples for question answering task.\n",
        "    \n",
        "    Args:\n",
        "        example: A batch of examples from the dataset\n",
        "        \n",
        "    Returns:\n",
        "        dict: Tokenized inputs with start/end positions for answers\n",
        "    \"\"\"\n",
        "    # Tokenize question + context\n",
        "    inputs = tokenizer(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        max_length=256,                      # Limit input length (faster training)\n",
        "        stride=128,                          # Overlap for long contexts\n",
        "        truncation=\"only_second\",            # Only truncate context, not question\n",
        "        return_offsets_mapping=True,         # Map tokens back to character positions\n",
        "        return_overflowing_tokens=True,      # Handle long contexts split into multiple chunks\n",
        "        padding=\"max_length\"                 # Pad to fixed size\n",
        "    )\n",
        "\n",
        "    # Helps us track which chunk came from which example\n",
        "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # Offsets map tokens to positions in original text\n",
        "    offsets = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "    starts, ends = [], []  # Stores token-level answer positions\n",
        "\n",
        "    # Loop through each chunk/tokenized input\n",
        "    for i, offset in enumerate(offsets):\n",
        "        ids = inputs[\"input_ids\"][i]                # Token IDs\n",
        "        cls = ids.index(tokenizer.cls_token_id)     # Index of [CLS] token\n",
        "        seq_ids = inputs.sequence_ids(i)            # 0 = question, 1 = context, None = padding\n",
        "        sample_idx = sample_mapping[i]              # Index of original sample\n",
        "        ans = dataset[\"train\"][sample_idx][\"answers\"]  # Original answer(s)\n",
        "\n",
        "        # Case: No answer (empty list)\n",
        "        if len(ans[\"answer_start\"]) == 0:\n",
        "            starts.append(cls)\n",
        "            ends.append(cls)\n",
        "        else:\n",
        "            # Get answer's start and end characters\n",
        "            start_char = ans[\"answer_start\"][0]\n",
        "            end_char = start_char + len(ans[\"text\"][0])\n",
        "\n",
        "            # Find token index for first context token\n",
        "            start_tok = 0\n",
        "            while seq_ids[start_tok] != 1:\n",
        "                start_tok += 1\n",
        "\n",
        "            # Find token index for last context token\n",
        "            end_tok = len(ids) - 1\n",
        "            while seq_ids[end_tok] != 1:\n",
        "                end_tok -= 1\n",
        "\n",
        "            # Check if answer lies within the current chunk\n",
        "            if not (offset[start_tok][0] <= start_char and offset[end_tok][1] >= end_char):\n",
        "                starts.append(cls)\n",
        "                ends.append(cls)\n",
        "            else:\n",
        "                # Move forward to exact token that includes start_char\n",
        "                while start_tok < len(offset) and offset[start_tok][0] <= start_char:\n",
        "                    start_tok += 1\n",
        "                starts.append(start_tok - 1)\n",
        "\n",
        "                # Move backward to exact token that includes end_char\n",
        "                while end_tok >= 0 and offset[end_tok][1] >= end_char:\n",
        "                    end_tok -= 1\n",
        "                ends.append(end_tok + 1)\n",
        "\n",
        "    # Add answer labels\n",
        "    inputs[\"start_positions\"] = starts\n",
        "    inputs[\"end_positions\"] = ends\n",
        "    return inputs\n",
        "\n",
        "print(\"‚úÖ Preprocessing function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Apply Preprocessing\n",
        "\n",
        "Now we'll apply the preprocessing function to the entire dataset. This step:\n",
        "- Tokenizes all questions and contexts\n",
        "- Converts text to PyTorch tensors\n",
        "- Prepares the data for efficient training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# ‚öôÔ∏è Apply Preprocessing to Entire Dataset\n",
        "# ---------------------------------------------\n",
        "print(\"üîÑ Applying preprocessing to dataset...\")\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names  # Keep only tokenized data, drop original text\n",
        ")\n",
        "tokenized.set_format(\"torch\")  # Convert to PyTorch tensors\n",
        "\n",
        "print(\"‚úÖ Preprocessing completed!\")\n",
        "print(f\"üìä Tokenized training examples: {len(tokenized['train']):,}\")\n",
        "print(f\"üìä Tokenized validation examples: {len(tokenized['validation']):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öñÔ∏è Training Setup\n",
        "\n",
        "We'll configure the training with optimal settings for fine-tuning:\n",
        "- **Learning Rate**: 3e-5 (standard for BERT-like models)\n",
        "- **Batch Size**: 12 (adjust based on your GPU memory)\n",
        "- **Mixed Precision**: FP16 for faster training\n",
        "- **Epochs**: 1 for demo (increase for better results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# ‚öñÔ∏è Data Collator\n",
        "# ---------------------------------------------\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# This will pad dynamically during training for efficiency\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# üõ† Define Training Settings\n",
        "# ---------------------------------------------\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fast-distilbert-qa\",       # Directory to save model checkpoints\n",
        "    eval_strategy=\"epoch\",                   # Evaluate at the end of each epoch\n",
        "    learning_rate=3e-5,                      # Fine-tuning learning rate\n",
        "    per_device_train_batch_size=12,          # Train batch size per GPU\n",
        "    per_device_eval_batch_size=12,           # Eval batch size per GPU\n",
        "    num_train_epochs=1,                      # Just 1 epoch for demo; increase for better results\n",
        "    weight_decay=0.01,                       # Regularization to avoid overfitting\n",
        "    fp16=True,                               # Use mixed precision (faster on GPU)\n",
        "    logging_steps=100,                       # Log every 100 steps\n",
        "    report_to=\"none\"                         # Disable external logging (e.g., WandB)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Training Time!\n",
        "\n",
        "The actual fine-tuning process begins here. The Trainer API will:\n",
        "1. Handle the training loop automatically\n",
        "2. Apply gradients and optimize the model\n",
        "3. Evaluate performance on the validation set\n",
        "4. Save checkpoints during training\n",
        "\n",
        "**Note**: Training time depends on your hardware. With GPU, this should take 30-60 minutes for 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# üîÅ Wrap Everything in Trainer API\n",
        "# ---------------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"‚è±Ô∏è This may take 30-60 minutes depending on your hardware...\")\n",
        "\n",
        "# ---------------------------------------------\n",
        "# üöÄ Start Training\n",
        "# ---------------------------------------------\n",
        "trainer.train()\n",
        "\n",
        "print(\"üéâ Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save the Model\n",
        "\n",
        "Finally, we'll save our fine-tuned model and tokenizer so you can use them later for inference or further training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "# üíæ Save Final Fine-Tuned Model\n",
        "# ---------------------------------------------\n",
        "print(\"üíæ Saving fine-tuned model and tokenizer...\")\n",
        "\n",
        "trainer.save_model(\"fast-distilbert-squad\")            # Save model weights\n",
        "tokenizer.save_pretrained(\"fast-distilbert-squad\")     # Save tokenizer\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer saved to 'fast-distilbert-squad' directory!\")\n",
        "print(\"üéØ You can now use this model for question answering tasks!\")\n",
        "print(\"‚úÖ Done!\")  # Training complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Next Steps\n",
        "\n",
        "Your fine-tuned DistilBERT model is ready! Here's what you can do next:\n",
        "\n",
        "1. **Test the Model**: Load the saved model and test it on new questions\n",
        "2. **Deploy**: Use the model in production applications\n",
        "3. **Further Training**: Train for more epochs to improve performance\n",
        "4. **Evaluation**: Run comprehensive evaluation metrics\n",
        "\n",
        "### üìñ How to Use Your Model\n",
        "\n",
        "```python\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
        "\n",
        "# Load your fine-tuned model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"fast-distilbert-squad\")\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"fast-distilbert-squad\")\n",
        "\n",
        "# Use for inference\n",
        "question = \"What is the capital of France?\"\n",
        "context = \"France is a country in Europe. Its capital city is Paris.\"\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "# Extract answer from outputs...\n",
        "```\n",
        "\n",
        "üéâ **Congratulations!** You've successfully fine-tuned a DistilBERT model for question answering!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
